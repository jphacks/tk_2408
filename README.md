# 自動動画翻訳 Vtuber プラットフォーム「Vany」

作品 URL：https://vany.vercel.app/

発表プレゼン URL：https://www.canva.com/design/DAGUtWS34GM/PYcP7rSeGGyJAEK1N_TNHw/edit?utm_content=DAGUtWS34GM&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton

デモ動画 URL：https://www.youtube.com/channel/UCd5zGSqKwh-Or43YOGG8F1g

注：現在 Flask プロジェクトをデプロイしていないため、上記 URL から動画アップロードをして翻訳をお試しいただくことはできません。

動画翻訳をお試しの際は、[実行手順セクション](https://github.com/jphacks/tk_2408/blob/main/README.md#%E5%8B%95%E7%94%BB%E7%BF%BB%E8%A8%B3%E5%AE%9F%E8%A1%8C%E6%89%8B%E9%A0%86)に記載の手順でローカルにサーバーを起動してください。

![Vany_yoko](https://github.com/user-attachments/assets/9d96905e-6daf-4fe8-a9b9-a9d7ce1eb3cd)

デモ動画(日本語)

https://github.com/user-attachments/assets/ca63223b-6e67-4800-95eb-137df4c20d2a

デモ動画(英語)

https://github.com/user-attachments/assets/7d125df0-193f-46fd-af83-341d0f106e78

機械学習側のフロー

1. 動画から音声を抽出(moviepy)
2. 抽出した音声を文字起こし(whiper)
3. 抽出した音声から声質を学習(elevenlabs)
4. 文字起こしを翻訳(openai)
5. 翻訳した文章を 3 で作成したモデルで TTS(elevenlabs)
6. 5 で作成した音声を元動画に結合(moviepy)

![diagram-export-2024-10-27-16_23_01](https://github.com/user-attachments/assets/4bc91e43-167b-4411-98aa-030b88d31d40)

## 製品概要

### 背景(製品開発のきっかけ、課題等）

現在 Vtuber はかなりのレッドオーシャンになっており、ユーザー獲得に苦しんでいる Vtuber が多くいます。
また、日本を中心に、アメリカ・韓国・中国と Vtuber が広がっていく中、言語の壁というものはここでも存在しています。
この課題と壁を Vany を使うことによって解決できるよう、動画をアップロードするだけで自分の声で多言語の動画に変換され投稿されるというプラットフォームを作ろうと思い開発いたしました。

### 製品説明（具体的な製品の説明）

この製品「Vany」は上記の課題を解決すべく開発された「動画プラットフォーム」です。
「Vany」は「Vtuber」の「anyvoice(色々な声)」という意味を込めて合わせたプロダクト名となっております。

海外に向けて発信したい Vtuber の方は、Vany にて動画を投稿することで、自分の声質のままさまざまな国の言語に翻訳された動画が投稿されるので、海外のユーザーにも認知してもらい新規のユーザーを獲得することができます。
色々な国の Vtuber を見たいユーザーは、自分の言語を設定して Vany で視聴するだけで、海外の Vtuber の動画も自分の言語に翻訳された動画として見ることができます。
このようなユーザーが増えることで、Vtuber は海外ユーザーの機会損失を減らし、ファンユーザーをより増やしていくことができます。

### 特長

#### 1. 特長 1

Vany の最も特徴的な機能が、「動画自動翻訳機能」です。
youtube のようにタイトルなどを入力後動画をアップロードするだけで、自動でその人の声を AI が学習し多言語に翻訳され、その人の声で翻訳された動画がアップロードされます。

#### 2. 特長 2

Vany では、API のみで実装しているため、GPU リソースも必要なく、安価かつ高速な動画翻訳を実現できます。

#### 3. 特長 3

Vany の特徴として、元動画とのずれが少ないことです。単純に文字起こしをすると言語によって文字列長が変わるため、動画がまだ途中なのに音が終わってしまったりします。
Vany では、文字認識の際にセンテンスごとの時間を記録しているため、元の動画通りに翻訳をしています。この特徴については、下に詳細な解説をしております。

### 解決出来ること

・個人 Vtuber や新設 Vtuber 事務所の新規ユーザー獲得

・言語の壁を超えた Vtuber の動画視聴

### 今後の展望

---機械学習部分の改良----

- **BGM の分離**

  音源分離モデルを使用

- **感情表現**

  日本語の生成精度が高く、感情も表現できる Style-Bert-VITS2 などを用いる

- **複数話者対応**

  話者分離モデルを組み合わせ、各話者の声を分けて処理する仕組みを導入する

- **日本語音声の生成精度の向上**

  Elevenlabs の日本語音声生成精度に課題があるため、他のモデルも比較検討する

---フロントエンド部分の改良---

- **プラットフォーム Web アプリとしての改善**

2 日間の開発では、デモに関わるページやデプロイ時に触っていただくページなど最低限のページを実装した形になりました。
今後は、他動画プラットフォーム同様の UI/UX を取り入れた完全な Web 動画プラットフォームアプリケーションを開発し、
ページ要件としても「コメント機能・検索機能・ギフト機能」といった動画プラットフォームとして必須のページと機能を実装することで、
Youtube 等と変わらない使い心地のまま、動画自動翻訳の価値を最大化していきます。
また、おすすめアルゴリズムにも挑戦し、さまざまな国の動画の中からおすすめ動画を選出できるよう取り組みます。

### 注力したこと（こだわり等）

**動画自動翻訳処理**

動画翻訳をするとなった時、以下の 2 つが課題となりました。

1. 文字起こしをしたのちに翻訳をすると、精度が落ちる&処理速度が遅くなる
2. 元動画への音声結合の際に、翻訳による文字量の違いから音がズレる

1 を解決するため、はじめ[kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)という、英語を日本語に高速に翻訳かつ文字起こしができるという魅力的なモデルを使用していました。

しかし、H100 を用いて回しても、動画の尺の数倍以上の時間がかかってしまう上、タイムスタンプを保持した形での翻訳ができず、2 の問題を解決できないという点と、GPU リソースを用いたデプロイも大変な点から、断念しました。

そのため、API だけで完結させたいと考え調べていたところ、OpenAI 社の API で直接音声から英語に翻訳して、かつタイムスタンプも維持するという[こちら](https://platform.openai.com/docs/api-reference/audio/verbose-json-object)のエンドポイントを見つけたため、こちらを使用しました。

結果的に、全てのフローを Pyhotn での API 経由で完結することができ、高速で簡単なデプロイ・運用が実現できます。

**課題解決に焦点を当てた要件定義**

Vtuber 市場の課題として、2022 年 11 月時点で 2 万人をこえる Vtuber が活動しており、個人 Vtuber や新規参入 Vtuber 事務所の新規ユーザー獲得が難しく、大手 Vtuber 事務所が独占しているという課題があります。
また、ホロライブ English のように、海外の Vtuber が活動しており動画を見たいという需要があるが、言語の壁があり動画を見ることができず Vtuber 側の機会損失が多く生まれています。
この課題を解決するためにどうしたらいいかと考えた上で、「アップロードするだけで、その人の声を学習し、翻訳された動画が投稿される」というプラットフォームを開発することで解決できると考え、この度実装いたしました。
また、製品の持続可能性という観点から、このサービスはプラットフォーム型の収益モデルを適用することができ、ユーザーを集めることで「プラットフォーム手数料・広告料」をいただくことができ、製品を開発し運営していうことが可能です。

## 開発技術

Next.js, React, PHP, Python, Flask

### 活用した技術

v0, Claude, Vercel

#### API・データ

Elevenlabs, OpenAI

### 独自技術

**元動画との音声ずれをなくす技術**

OpenAI 社の API を用いることで、タイムスタンプ付きの文字認識を利用できたため、それを用いて、動画との音声ずれを最小限にしました。
タイムスタンプを用いてどのようにずれを無くしたのかについて解説させていただきます。
まず、タイムスタンプは OpenAI 社の API 経由で以下の形式で取得できます。

```
{
  "duration": 9.399999618530273,
  "language": "english",
  "text": "You're such .. all?",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.9008601307868958,
      "compression_ratio": 1.0537633895874023,
      "end": 2.359999895095825,
      "no_speech_prob": 0.03196346014738083,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " You're such a troublemaker, Usagi-san!",
      "tokens": […]
    },
...
```

上記を見ていただくとわかるように、センテンスごとに分割され、それぞれ start と end の時間が記載されています。
そのため、以下のロジックを用いて音声編集をしたのち、各チャンクを結合することで、音声のずれを無くしました。

**元動画の音声 > 生成音声の場合**
生成音声の前後に無音音声を追加する
追加する無音音声の長さは、それぞれ
`元動画の音声 - 生成音声`
の長さの無音音声を各チャンクの後に結合

例えば、生成音声が 4 秒で、元動画での長さが 5 秒の場合の時は
生成音声の前後に 0.5 秒の無音区間を追加

**生成音声 > 元動画の音声の場合**
`生成音声 / 元動画の音声`
を速度因子として生成音声に乗算する
例えば、生成音声が 5 秒で、元動画での長さが 4 秒の場合の時は
5 / 4 = 1.25 倍
に再生速度を調整

## 動画翻訳実行手順(テスト用)

### Python 実行テスト手順

1. `.env`ファイルを作成し、OPENAI_API_KEY と ELEVENLABS_API_KEY を記載。

2. `pip install -r requirements.txt`を実行

3. `VIDEO_PATH`に入力動画を配置し、`EXTRACTED_AUDIO_PATH`に適当なパスを設定し、`python app.py`を実行すると、その声の翻訳動画が`OUTPUT_MOVIE_PATH`に配置される。

注：Elevenlabs の請求アカウントを登録しないと利用できません。

### Flask 実行手順

1. 上記設定後、`python main.py`を実行

2. Web サイト上で動画アップロードすると翻訳される

※実際は Docker 経由で GCP にデプロイしているため、上記手順はテスト用です。

### フロントエンド実行手順

1. `cd frontend`

2. `yarn`

3. `yarn dev`

4. [ここ](http://localhost:3000/vtuber/login)にアクセスし、

```
メール：3848hiro@gmail.com
パスワード：123456
```

5. [ここ](http://localhost:3000/upload)にアクセスし、サムネイル画像やビデオをアップロード

6. 翻訳動画が生成、投稿される

注：Flask を実行した状態でないと、動画のアップロード処理が動作しません。
