# 自動動画翻訳Vtuberプラットフォーム「Vany」

作品URL：https://vany.vercel.app/
発表プレゼンURL：https://www.canva.com/design/DAGUtWS34GM/PYcP7rSeGGyJAEK1N_TNHw/edit?utm_content=DAGUtWS34GM&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton

注：現在Flaskプロジェクトをデプロイしていないため、動画のアップロードはできません。

動画翻訳をお試しの際は、[実行手順セクション](https://github.com/jphacks/tk_2408/blob/main/README.md#%E5%8B%95%E7%94%BB%E7%BF%BB%E8%A8%B3%E5%AE%9F%E8%A1%8C%E6%89%8B%E9%A0%86)に記載の手順でローカルにサーバーを起動してください。

![Vany_yoko](https://github.com/user-attachments/assets/9d96905e-6daf-4fe8-a9b9-a9d7ce1eb3cd)

デモ動画(日本語)

https://github.com/user-attachments/assets/ca63223b-6e67-4800-95eb-137df4c20d2a

デモ動画(英語)

https://github.com/user-attachments/assets/7d125df0-193f-46fd-af83-341d0f106e78

機械学習側のフロー

1. 動画から音声を抽出(moviepy)
2. 抽出した音声を文字起こし(whiper)
3. 抽出した音声から声質を学習(elevenlabs)
4. 文字起こしを翻訳(openai)
5. 翻訳した文章を3で作成したモデルでTTS(elevenlabs)
6. 5で作成した音声を元動画に結合(moviepy)

![diagram-export-2024-10-27-16_23_01](https://github.com/user-attachments/assets/4bc91e43-167b-4411-98aa-030b88d31d40)


## 製品概要

### 背景(製品開発のきっかけ、課題等）
現在Vtuberはかなりのレッドオーシャンになっており、ユーザー獲得に苦しんでいるVtuberが多くいます。
また、日本を中心に、アメリカ・韓国・中国とVtuberが広がっていく中、言語の壁というものはここでも存在しています。
この課題と壁をVanyを使うことによって解決できるよう、動画をアップロードするだけで自分の声で多言語の動画に変換され投稿されるというプラットフォームを作ろうと思い開発いたしました。

### 製品説明（具体的な製品の説明）
この製品「Vany」は上記の課題を解決すべく開発された「動画プラットフォーム」です。
「Vany」は「Vtuber」の「anyvoice(色々な声)」という意味を込めて合わせたプロダクト名となっております。

海外に向けて発信したいVtuberの方は、Vanyにて動画を投稿することで、自分の声質のままさまざまな国の言語に翻訳された動画が投稿されるので、海外のユーザーにも認知してもらい新規のユーザーを獲得することができます。
色々な国のVtuberを見たいユーザーは、自分の言語を設定してVanyで視聴するだけで、海外のVtuberの動画も自分の言語に翻訳された動画として見ることができます。
このようなユーザーが増えることで、Vtuberは海外ユーザーの機会損失を減らし、ファンユーザーをより増やしていくことができます。


### 特長

#### 1. 特長 1
Vanyの最も特徴的な機能が、「動画自動翻訳機能」です。
youtubeのようにタイトルなどを入力後動画をアップロードするだけで、自動でその人の声をAIが学習し多言語に翻訳され、その人の声で翻訳された動画がアップロードされます。

#### 2. 特長 2
Vanyでは、APIのみで実装しているため、GPUリソースも必要なく、安価かつ高速な動画翻訳を実現できます。

#### 3. 特長 3
Vanyの特徴として、元動画とのずれが少ないことです。単純に文字起こしをすると言語によって文字列長が変わるため、動画がまだ途中なのに音が終わってしまったりします。
Vanyでは、文字認識の際にセンテンスごとの時間を記録しているため、元の動画通りに翻訳をしています。この特徴については、下に詳細な解説をしております。

### 解決出来ること

・個人Vtuberや新設Vtuber事務所の新規ユーザー獲得

・言語の壁を超えたVtuberの動画視聴

### 今後の展望

**動画自動翻訳処理の改良**
現在使用しているElevenlabsでは、日本語の音声生成の精度が低いため、今回は日本語→英語間の翻訳が主として発表させていただきました。

-  **BGMの分離**

   音源分離モデルを使用

-  **感情表現**

   日本語の生成精度が高く、感情も表現できるStyle-Bert-VITS2などを用いる

- **複数話者対応**

   話者分離モデルを組み合わせ、各話者の声を分けて処理する仕組みを導入する

- **日本語音声の生成精度の向上**

  Elevenlabsの日本語音声生成精度に課題があるため、他のモデルも比較検討する


**プラットフォームWebアプリとしての改善**
2日間の開発では、デモに関わるページやデプロイ時に触っていただくページなど最低限のページを実装した形になりました。
今後は、他動画プラットフォーム同様のUI/UXを取り入れた完全なWeb動画プラットフォームアプリケーションを開発し、
ページ要件としても「コメント機能・検索機能・ギフト機能」といった動画プラットフォームとして必須のページと機能を実装することで、
Youtube等と変わらない使い心地のまま、動画自動翻訳の価値を最大化していきます。
また、おすすめアルゴリズムにも挑戦し、さまざまな国の動画の中からおすすめ動画を選出できるよう取り組みます。

### 注力したこと（こだわり等）
**動画自動翻訳処理良**
動画翻訳をするとなった時、以下の2つが課題となりました。
1. 文字起こしをしたのちに翻訳をすると、精度が落ちる&処理速度が遅くなる。
2. 元動画への音声結合の際に、翻訳による文字量の違いから音がズレる

1を解決するため、はじめ[kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)という、英語を日本語に高速に翻訳かつ文字起こしができるという魅力的なモデルを使用していました。

しかし、H100を用いて回しても、動画の尺の数倍以上の時間がかかってしまう上、タイムスタンプを保持した形での翻訳ができず、2の問題を解決できないという点と、GPUリソースを用いたデプロイも大変な点から、断念しました。

そのため、APIだけで完結させたいと考え調べていたところ、OpenAI社のAPIで直接音声から英語に翻訳して、かつタイムスタンプも維持するという[こちら](https://platform.openai.com/docs/api-reference/audio/verbose-json-object)のエンドポイントを見つけたため、こちらを使用しました。

結果的に、全てのフローをPyhotnでのAPI経由で完結することができ、高速で簡単なデプロイ・運用が実現できます。

**課題解決に焦点を当てた要件定義**
Vtuber市場の課題として、2022年11月時点で2万人をこえるVtuberが活動しており、個人Vtuberや新規参入Vtuber事務所の新規ユーザー獲得が難しく、大手Vtuber事務所が独占しているという課題があります。
また、ホロライブ Englishのように、海外のVtuberが活動しており動画を見たいという需要があるが、言語の壁があり動画を見ることができずVtuber側の機会損失が多く生まれています。
この課題を解決するためにどうしたらいいかと考えた上で、「アップロードするだけで、その人の声を学習し、翻訳された動画が投稿される」というプラットフォームを開発することで解決できると考え、この度実装いたしました。
また、製品の持続可能性という観点から、このサービスはプラットフォーム型の収益モデルを適用することができ、ユーザーを集めることで「プラットフォーム手数料・広告料」をいただくことができ、製品を開発し運営していうことが可能です。


## 開発技術
Next.js, React, PHP, Python, Flask

### 活用した技術
v0, Claude, Vercel

#### API・データ
Elevenlabs, OpenAI

### 独自技術
**元動画との音声ずれをなくす技術**

OpenAI社のAPIを用いることで、タイムスタンプ付きの文字認識を利用できたため、それを用いて、動画との音声ずれを最小限にしました。
タイムスタンプを用いてどのようにずれを無くしたのかについて解説させていただきます。
まず、タイムスタンプはOpenAI社のAPI経由で以下の形式で取得できます。

```
{
  "duration": 9.399999618530273,
  "language": "english",
  "text": "You're such .. all?",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.9008601307868958,
      "compression_ratio": 1.0537633895874023,
      "end": 2.359999895095825,
      "no_speech_prob": 0.03196346014738083,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " You're such a troublemaker, Usagi-san!",
      "tokens": […]
    },
...
```

上記を見ていただくとわかるように、センテンスごとに分割され、それぞれstartとendの時間が記載されています。
そのため、以下のロジックを用いて音声編集をしたのち、各チャンクを結合することで、音声のずれを無くしました。

**元動画の音声 > 生成音声の場合**
生成音声の前後に無音音声を追加する
追加する無音音声の長さは、それぞれ
`元動画の音声 - 生成音声`
の長さの無音音声を各チャンクの後に結合

例えば、生成音声が4秒で、元動画での長さが5秒の場合の時は
生成音声の前後に0.5秒の無音区間を追加

**生成音声 > 元動画の音声の場合**
`生成音声 / 元動画の音声`
を速度因子として生成音声に乗算する
例えば、生成音声が5秒で、元動画での長さが4秒の場合の時は
5 / 4 = 1.25倍
に再生速度を調整

## 動画翻訳実行手順

### Python実行手順

1. `.env`ファイルを作成し、OPENAI_API_KEYとELEVENLABS_API_KEYを記載。

2. `pip install -r requirements.txt`を実行

3. `VIDEO_PATH`に入力動画を配置し、`EXTRACTED_AUDIO_PATH`に適当なパスを設定し、`python app.py`を実行すると、その声の翻訳動画が`OUTPUT_MOVIE_PATH`に配置される。

注：Elevenlabsの請求アカウントを登録しないと利用できません。

### Flask実行手順

1. 上記設定後、`python flask_main.py`を実行

2. Webサイト上で動画アップロードすると翻訳される

### Next.js実行手順

1. `cd frontend`

2. `npm install`

3. `npx next dev`

4. [このURL](http://localhost:3000/vtuber/login)にアクセスし、
```
メール：3848hiro@gmail.com
パスワード：123456
```

5. [このURL](http://localhost:3000/upload)にアクセスし、サムネイル画像やビデオをアップロード

6. 翻訳動画が生成、投稿される

注：Flaskを実行した状態でないと、動画のアップロード処理が動作しません。
