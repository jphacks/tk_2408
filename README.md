# 自動動画翻訳Vtuberプラットフォーム「Vany」

作品URL：https://vany.vercel.app/

![Vany_yoko](https://github.com/user-attachments/assets/9d96905e-6daf-4fe8-a9b9-a9d7ce1eb3cd)

デモ動画(日本語)

https://github.com/user-attachments/assets/ca63223b-6e67-4800-95eb-137df4c20d2a

デモ動画(英語)

https://github.com/user-attachments/assets/7d125df0-193f-46fd-af83-341d0f106e78

機械学習側のフロー

1. 動画から音声を抽出(moviepy)
2. 抽出した音声を文字起こし(whiper)
3. 抽出した音声から声質を学習(elevenlabs)
4. 文字起こしを翻訳(openai)
5. 翻訳した文章を3で作成したモデルでTTS(elevenlabs)
6. 5で作成した音声を元動画に結合(moviepy)

![diagram-export-2024-10-27-16_23_01](https://github.com/user-attachments/assets/4bc91e43-167b-4411-98aa-030b88d31d40)


## 製品概要

### 背景(製品開発のきっかけ、課題等）
現在Vtuberはかなりのレッドオーシャンになっており、ユーザー獲得に苦しんでいるVtuberが多くいます。
また、日本を中心に、アメリカ・韓国・中国とVtuberが広がっていく中、言語の壁というものはここでも存在しています。
この課題と壁をVanyを使うことによって解決できるよう、動画をアップロードするだけで自分の声で多言語の動画に変換され投稿されるというプラットフォームを作ろうと思い開発いたしました。

### 製品説明（具体的な製品の説明）
この製品「Vany」は上記の課題を解決すべく開発された「動画プラットフォーム」です。
「Vany」は「Vtuber」の「anyvoice(色々な声)」という意味を込めて合わせたプロダクト名となっております。

海外に向けて発信したいVtuberの方は、Vanyにて動画を投稿することで、自分の声質のままさまざまな国の言語に翻訳された動画が投稿されるので、海外のユーザーにも認知してもらい新規のユーザーを獲得することができます。
色々な国のVtuberを見たいユーザーは、自分の言語を設定してVanyで視聴するだけで、海外のVtuberの動画も自分の言語に翻訳された動画として見ることができます。


### 特長

#### 1. 特長 1
Vanyの最も特徴的な機能が、「動画自動翻訳機能」です。
youtubeのようにタイトルなどを入力後動画をアップロードするだけで、自動でその人の声をAIが学習し多言語に翻訳され、その人の声で翻訳された動画がアップロードされます。

#### 2. 特長 2
Vanyでは、APIのみで実装しているため、GPUリソースも必要なく、高速な動画翻訳を実現できます。

#### 3. 特長 3
Vanyの特徴として、元動画とのずれが少ないことです。単純に文字起こしをすると言語によって文字列長が変わるため、動画がまだ途中なのに音が終わってしまったりします。
Vanyでは、文字認識の際にセンテンスごとの時間を記録しているため、元の動画通りに翻訳をしています。この特徴については、下に詳細な解説をしております。

### 解決出来ること
・個人Vtuberや新設Vtuber事務所の新規ユーザー獲得
・言語の壁を超えたVtuberの動画視聴

### 今後の展望
現在使用しているElevenlabsでは、日本語の音声生成の精度が低いため、今回は日本語→英語間の翻訳が主として発表させていただきました。

しかし、世界には多くのVTuberファンがいるため、多言語への対応をしたいと考えています。

また、現在は音声を文字ベースで生成しているため、BGMと感情の再現ができておりません。

BGMについてはシンプルな音源分離モデルで可能と考えていますが、感情は少し難しいです。

ただ、Style-Bert-VITS2を使うことで高精度な日本語音声の生成と感情の表現ができることは確認できたため、それを使用した形式で試してみたいです。

また、複数人でのコラボ配信の際にもそれぞれの声に対応できるよう、話者分離のモデルも組み合わせたいと考えています。

### 注力したこと（こだわり等）
動画翻訳をするとなった時、以下の2つが課題となりました。
1. 文字起こしをしたのちに翻訳をすると、精度が落ちる&処理速度が遅くなる。
2. 元動画への音声結合の際に、翻訳による文字量の違いから音がズレる

1を解決するため、はじめ[kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)という英語→日本語に高速に英翻訳して文字起こしできるモデルを使用していました。

しかし、数分の動画でもH100で1分以上かかってしまった上、タイムスタンプを保持した形での翻訳ができず、2の問題を解決できませんんでした。

GPUリソースを用いたデプロイも大変なので断念しました。

そのため、APIだけで完結させたいと考え調べていたところ、OpenAI社のAPIで直接音声から英語に翻訳して、かつタイムスタンプも維持するという[こちら](https://platform.openai.com/docs/api-reference/audio/verbose-json-object)のエンドポイントを見つけたため、こちらを使用しました。

結果的に、全てのフローをPyhotnでのAPI経由で完結することができ、高速かつ簡単なデプロイ&運用が実現できました。

## 開発技術
フロントエンド・バックエンド: Next.js, React, PHP, Python, Flask

### 活用した技術
サービス・AIモデル: v0, Claude, Vercel

#### API・データ
Elevenlabs, OpenAI

### 独自技術
**元動画との音声ずれをなくす技術**
OpenAI社のAPIを用いることで、タイムスタンプ付きの文字認識を利用できたため、それを用いて、動画との音声ずれを最小限にしました。
タイムスタンプを用いてどのようにずれを無くしたのかについて解説させていただきます。
まず、タイムスタンプはOpenAI社のAPI経由で以下の形式で取得できます。

```
{
  "duration": 9.399999618530273,
  "language": "english",
  "text": "You're such .. all?",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.9008601307868958,
      "compression_ratio": 1.0537633895874023,
      "end": 2.359999895095825,
      "no_speech_prob": 0.03196346014738083,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " You're such a troublemaker, Usagi-san!",
      "tokens": […]
    },
...
```

上記を見ていただくとわかるように、センテンスごとに分割され、それぞれstartとendの時間が記載されています。
そのため、以下のロジックを用いて音声編集をしたのち、各チャンクを結合することで、音声のずれを無くしました。

**元動画の音声 > 生成音声の場合**
生成音声の前後に無音音声を追加する
追加する無音音声の長さは、それぞれ
`元動画の音声 - 生成音声`
の長さの無音音声を各チャンクの後に結合

例えば、生成音声が4秒で、元動画での長さが5秒の場合の時は
生成音声の前後に0.5秒の無音区間を追加

**生成音声 > 元動画の音声の場合**
`生成音声 / 元動画の音声`
を速度因子として生成音声に乗算する
例えば、生成音声が5秒で、元動画での長さが4秒の場合の時は
5 / 4 = 1.25倍
に再生速度を調整

#### ハッカソンで開発した独自機能・技術

- 独自で開発したものの内容をこちらに記載してください
- 特に力を入れた部分をファイルリンク、または commit_id を記載してください。
https://github.com/jphacks/tk_2408/issues/6
